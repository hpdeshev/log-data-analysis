{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "import io\n",
    "import itertools\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "from shutil import rmtree\n",
    "from tempfile import mkdtemp\n",
    "from textwrap import TextWrapper\n",
    "from typing import Any\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import constant, Tensor\n",
    "from tensorflow.data import AUTOTUNE, Dataset\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "  Dense, Embedding, GRU, Layer, TextVectorization\n",
    ")\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import (\n",
    "  deserialize_keras_object, register_keras_serializable,\n",
    "  serialize_keras_object, set_random_seed,\n",
    ")\n",
    "from typing import override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPY53Wpwd128"
   },
   "source": [
    "# Demonstration of a generic hands-on deep learning method for anomaly detection in log data\n",
    "## Introduction\n",
    "### Motivation\n",
    "\n",
    "This notebook demonstrates how unexpected subsequences (anomalies) can be found in sequential text data. The example implementation uses [software logs](https://en.m.wikipedia.org/wiki/Logging_(computing)).\n",
    "\n",
    "Logs can be very helpful in understanding the behavior of a new system, application or environment we have only recently started to work with. The latter situation involves some incremental learning process - from human and ... possibly machine standpoint. That is, machine learning (ML) can be utilized as a powerful tool in log data analysis.\n",
    "\n",
    "Log messages, in general, are very specific to the activities being logged and thus can contain numeric data, so anomaly detection methods based on [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing), including the [bag-of-words (BoW)](https://en.wikipedia.org/wiki/Bag-of-words_model) model, can be applied only to a limited extent. For the purpose of genericity, the most important *component* of a log to be analyzed by this notebook's method is the *sequence* of log messages.\n",
    "\n",
    "Focusing on the log message *sequences*, there are 2 main tasks involved in preparing log data for ML-aided analysis:\n",
    "1. cleaning, filtering and sorting - to obtain only the log messages of interest;\n",
    "2. classification or clusterization of the selected log messages into messages types to be processed as a sequence.\n",
    "\n",
    "Both task 1 and task 2, but especially task 2, can be accomplished via ML.\n",
    "\n",
    "Task 1 is important in filtering out only the relevant log information, especially when the log is huge in size (e.g., gigabytes). Regular expressions or even ML can be utilized for solving this task.\n",
    "\n",
    "Task 2 is based on task 1, and for solving this task some ML text classification or clustering algorithm can be utilized, as compared to using the pure regular expressions approach.\n",
    "\n",
    "After task 1 and task 2 are completed, a log is converted to a sequence of *message types* rather than messages.\n",
    "\n",
    "|  Log             |\n",
    "|------------------|\n",
    "|  Message type 1  |\n",
    "|  ...             |\n",
    "|  Message type n  |\n",
    "\n",
    "\n",
    "However, mentioning *message type* is just for clarification regarding how the method works, so in this notebook a log entry is just called *log message*.\n",
    "\n",
    "### Method\n",
    "\n",
    "As per [A survey on the application of deep learning for anomaly detection in logs](#a-survey-on-the-application-of-deep-learning-for-anomaly-detection-in-logs), there exist various approaches for anomaly detection in sequential text data. **In particular, this notebook demonstrates a hands-on, generic in its simplicity, method for finding anomalies in a potentially large amount of logs by utilizing [k-means clustering](#k-means-clustering), regular expressions and [recurrent neural network (`RNN`)](#recurrent-neural-network).**\n",
    "\n",
    "The implementation includes three main tasks:\n",
    "1. application of an [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning) clusterization method via [k-means clustering](#k-means-clustering), based on [singular value decomposition (`SVD`)](#singular-value-decomposition) on potentially high-dimensional sparse data;\n",
    "1. transformation of the clustered log message data into unique per-cluster regular expression patterns that represent each log message as input to the `RNN` model;\n",
    "1. application of a [semi-supervised](https://en.wikipedia.org/wiki/Weak_supervision) anomaly detection method via `RNN`, based on the following step-by-step algorithm:\n",
    "    1. training process to learn possible log message sequences in already available normal log data (the supervised part);\n",
    "    1. next log message predictions on new log data;\n",
    "    1. [optional] if in step 2 a next log message cannot be predicted based on what is learned in step 1, a human intervention is needed to determine whether this is an anomaly or just a new log message to be learned (the unsupervised part);\n",
    "    1. [optional] if in step 3 are found new normal log messages, go to step 1, else: go to step 2.\n",
    "\n",
    "The implementation of the `BoW` log clusterization model is based on [Scikit-learn](#scikit-learn).\n",
    "The implementation of the `RNN` anomaly detection model is based on [TensorFlow](#tensorflow).\n",
    "\n",
    "In this notebook no hyperparameter tuning is performed because the main purpose of the notebook is to demonstrate a working example.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "This notebook's generic method does not perform any timestamp-related log data preprocessing as this would cause noise in the data and potentially incorrect results. This is why there is a setting that specifies removal of initial token(s) per log line, where is typically the place of a timestamp.\n",
    "\n",
    "### Other log message grouping methods\n",
    "\n",
    "The demonstrated clusterization method is a coarse, generic and basically *unsupervised* one. Apart from tuning the number of clusters found using the `k-means` method, via additional configuration it is possible to perform fine-grained grouping on log messages, for example, by at least partial log *message labeling* combined with *label-spreading*. Also, a not ML-based approach with regular expressions is possible. However, such alternative methods tend to require expert-level assistance and thus do not represent a generic, autonomous solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORWVgBxMd12-"
   },
   "source": [
    "## System setup\n",
    "\n",
    "[Scikit-learn](#scikit-learn), [TensorFlow](#tensorflow) and [Keras 2](#keras) are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gO5S76td12_"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rx8ohkJd12_"
   },
   "outputs": [],
   "source": [
    "# Common\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data\n",
    "CACHE_DIR = mkdtemp()\n",
    "COLUMN_INDICES_TO_DROP = [0]  # E.g., date/time.\n",
    "DATA_DIR = \"data\"\n",
    "DATASET_FILE = \"10492770.zip\"\n",
    "DATASET_FILES = [\n",
    "  \"Authentication_Logs_logs.csv\",\n",
    "  \"Cloud-Based_Services_logs.csv\",\n",
    "  \"Operating_System_Logs_logs.csv\",\n",
    "  \"Server_Logs_logs.csv\",\n",
    "  \"Syslog_Data_logs.csv\",\n",
    "]\n",
    "DATASET_URL = (\n",
    "  \"https://zenodo.org/api/records/\"\n",
    "  f\"{os.path.splitext(DATASET_FILE)[0]}/files-archive\"\n",
    ")\n",
    "SKIPPED_LOG_SEQUENCE = (2, 0)  # Indices from DATASET_FILES.\n",
    "LOG_SIZE = 100_000\n",
    "SUBSEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 10_000\n",
    "\n",
    "# Modeling\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "BOW_MODEL_NAME = \"log_data_clusterizer.pkl\"\n",
    "BOW_MODEL_PATH = (\n",
    "  Path() / MODELS_DIR / BOW_MODEL_NAME\n",
    ")\n",
    "KMEANS_RETRIES = 10\n",
    "MIN_EXPLAINED_VARIANCE_RATIO = 0.95\n",
    "MIN_CLUSTERS = 2\n",
    "CLUSTERS = 5\n",
    "MAX_CLUSTERS = 8\n",
    "\n",
    "RNN_MODEL_NAME = \"text_sequence_analyzer.keras\"\n",
    "RNN_MODEL_PATH = (\n",
    "  Path() / MODELS_DIR / RNN_MODEL_NAME\n",
    ")\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Output formatting\n",
    "SEPARATOR = \" \" * 3\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10_000\n",
    "EARLY_STOP_PATIENCE = 3\n",
    "EPOCHS = 100\n",
    "\n",
    "# Testing\n",
    "NEXT_TOKEN_PROB_THRESHOLD = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaidxwV1d12_"
   },
   "source": [
    "## Log generator\n",
    "\n",
    "The `retrieve_dataset` function downloads from `DATASET_URL` to `DATA_DIR` the [Comprehensive Network Logs Dataset for Multi-Device Analysis](#comprehensive-network-logs-dataset-for-multi-device-analysis) log dataset.\n",
    "\n",
    "The generated log is a random mix of messages from `DATASET_FILES` which are part of the log dataset.\n",
    "\n",
    "Intentional dataset-to-dataset sequence gaps are left as per the dataset indices from `SKIPPED_LOG_SEQUENCE`, so that to be possible to reproduce anomalies based on learned incomplete sequences. It must be noted that `SKIPPED_LOG_SEQUENCE` can be applied reliably for more than 2 `DATASET_FILES`.\n",
    "\n",
    "As this notebook's method does not perform timestamp-related processing, the initial column, which typically contains logged date/time, is removed by default. Also, *NaN* data is removed, if any.\n",
    "\n",
    "The `generate_log` function generates a log of specified size. To be followed a typical case, the generated log is represented as a whole text of lines where each line corresponds to a log message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEsJBEz9d13A"
   },
   "outputs": [],
   "source": [
    "def retrieve_dataset() -> None:\n",
    "  ds_path = Path() / DATA_DIR / DATASET_FILE\n",
    "  if not Path.exists(ds_path):\n",
    "    response = requests.get(DATASET_URL)\n",
    "    zip_fileobj = io.BytesIO(response.content)\n",
    "    with open(ds_path, \"wb\") as file:\n",
    "      file.write(zip_fileobj.getbuffer())\n",
    "    with ZipFile(zip_fileobj) as zfile:\n",
    "      zfile.extractall(ds_path.parent)\n",
    "\n",
    "\n",
    "class LogGenerator:\n",
    "  \"\"\"Log generator implemented as a callable.\n",
    "\n",
    "  The generated log is a random mix of messages from `DATASET_FILES`.\n",
    "\n",
    "  Intentional dataset-to-dataset sequence gaps are left as per the dataset\n",
    "  indices from `SKIPPED_LOG_SEQUENCE`, so that to be possible to reproduce\n",
    "  anomalies based on learned incomplete sequences. It must be noted that\n",
    "  `SKIPPED_LOG_SEQUENCE` works reliably for more than 2 `DATASET_FILES`.\n",
    "\n",
    "  For each log dataset, the initial column, which typically contains logged\n",
    "  date/time, is removed by default. Also, *NaN* data is removed, if any.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self._datasets: list[list[str]] = []\n",
    "    for filename in DATASET_FILES:\n",
    "      dataset = pd.read_csv(Path() / DATA_DIR / filename)\n",
    "      dataset = dataset.drop(\n",
    "        columns=dataset.columns[COLUMN_INDICES_TO_DROP]\n",
    "      )\n",
    "      dataset = dataset.dropna()\n",
    "      dataset_as_list = dataset.apply(\n",
    "        lambda row: \" \".join([str(row[col]).strip() for col in dataset]),\n",
    "        axis=1,\n",
    "      ).to_list()\n",
    "      self._datasets.append(dataset_as_list)\n",
    "    self._ds_idx = -1\n",
    "    self._ds_pos = [-1] * len(self._datasets)\n",
    "\n",
    "  def __iter__(self) -> \"LogGenerator\":\n",
    "    return self\n",
    "\n",
    "  def __next__(self) -> str:\n",
    "    ds_idx = random.randint(0, len(self._datasets)-1)\n",
    "    while (self._ds_idx, ds_idx) == SKIPPED_LOG_SEQUENCE:\n",
    "      ds_idx = random.randint(0, len(self._datasets)-1)\n",
    "    self._ds_idx = ds_idx\n",
    "    self._ds_pos[ds_idx] += 1\n",
    "    if self._ds_pos[ds_idx] == len(self._datasets[ds_idx]):\n",
    "      self._ds_pos[ds_idx] = 0\n",
    "    return self._datasets[ds_idx][self._ds_pos[ds_idx]]\n",
    "\n",
    "\n",
    "def generate_log(\n",
    "  log_gen: LogGenerator,\n",
    "  log_size: int = LOG_SIZE,\n",
    ") -> list[str]:\n",
    "  return [\"\\n\".join([next(log_gen) for _ in range(log_size)])]\n",
    "\n",
    "\n",
    "set_random_seed(RANDOM_SEED)\n",
    "retrieve_dataset()\n",
    "log_gen = LogGenerator()\n",
    "log_text = generate_log(log_gen)\n",
    "log_lines = log_text[0].splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHMX95SLd13B"
   },
   "source": [
    "## Log message types clustering\n",
    "\n",
    "Log message types clustering is realized by a `BoW` model which is a [Scikit-learn](#scikit-learn) pipeline consisting of:\n",
    "- an `sklearn.feature_extraction.text.CountVectorizer` instance for per-message feature extraction;\n",
    "- an `sklearn.decomposition.TruncatedSVD` instance for dimensionality reduction via `SVD`, important for a stable clustering process;\n",
    "- an `sklearn.preprocessing.Normalizer` instance for normalization of the `SVD` data, important for a stable clustering process;\n",
    "- an `sklearn.cluster.KMeans` instance to perform the clustering process.\n",
    "\n",
    "An optimal number of `SVD` components is determined by the `get_svd_components_count` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhWTdghId13B"
   },
   "outputs": [],
   "source": [
    "if not Path.exists(BOW_MODEL_PATH):\n",
    "  def get_svd_components_count(\n",
    "    vectorizer: CountVectorizer,\n",
    "    X: Iterable[str],\n",
    "  ) -> int:\n",
    "    \"\"\"Returns an optimal number of SVD components for specified data.\n",
    "\n",
    "    Args:\n",
    "      vectorizer: Transforms `X` to a matrix of frequency counts per feature.\n",
    "      X: Iterable for log messages.\n",
    "\n",
    "    Returns:\n",
    "      A number of SVD components corresponding to\n",
    "      `MIN_EXPLAINED_VARIANCE_RATIO`.\n",
    "    \"\"\"\n",
    "    X_vect = vectorizer.fit_transform(X)\n",
    "    svd = TruncatedSVD(\n",
    "      n_components=len(vectorizer.vocabulary_),\n",
    "      random_state=RANDOM_SEED,\n",
    "    )\n",
    "    svd.fit(X_vect)\n",
    "    explained_variance_ratio = svd.explained_variance_ratio_\n",
    "    explained_variance_ratio = explained_variance_ratio[\n",
    "      np.argsort(explained_variance_ratio)\n",
    "    ][::-1]\n",
    "    return int(np.argmax(\n",
    "      np.cumsum(explained_variance_ratio) >= MIN_EXPLAINED_VARIANCE_RATIO\n",
    "    )) + 1\n",
    "\n",
    "  vectorizer = CountVectorizer(\n",
    "    lowercase=False, tokenizer=str.split, token_pattern=None\n",
    "  )\n",
    "  log_data_clusterizer = make_pipeline(\n",
    "    vectorizer,\n",
    "    TruncatedSVD(\n",
    "      n_components=get_svd_components_count(vectorizer, log_lines),\n",
    "      random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    Normalizer(copy=False),\n",
    "    KMeans(n_init=KMEANS_RETRIES, random_state=RANDOM_SEED),\n",
    "    memory=CACHE_DIR,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjtoiAVTd13C"
   },
   "source": [
    "### Number of clusters\n",
    "\n",
    "By application of the *elbow* method, `CLUSTERS` clusters are selected. Even though preset in the [configuration section](#configuration), `CLUSTERS` is to be selected here based on an *elbow*-like curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnoAxw_ld13C"
   },
   "outputs": [],
   "source": [
    "if not Path.exists(BOW_MODEL_PATH):\n",
    "  def train_clusterizer(model: Pipeline) -> None:\n",
    "    k_inertias: list[float] = []\n",
    "    k_range = np.r_[MIN_CLUSTERS:CLUSTERS, CLUSTERS+1:MAX_CLUSTERS]\n",
    "    for k in k_range:\n",
    "      model.set_params(kmeans__n_clusters=k)\n",
    "      model.fit(log_lines)\n",
    "      k_inertias.append(model.named_steps[\"kmeans\"].inertia_)\n",
    "    plt.scatter(k_range, k_inertias, c=\"blue\")\n",
    "\n",
    "    model.set_params(kmeans__n_clusters=CLUSTERS)\n",
    "    model.fit(log_lines)\n",
    "    plt.scatter(\n",
    "      [CLUSTERS], model.named_steps[\"kmeans\"].inertia_, c=\"red\",\n",
    "      label=f\"Selected K={CLUSTERS}\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.xticks(np.concatenate((k_range, [CLUSTERS])))\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  train_clusterizer(log_data_clusterizer)\n",
    "  rmtree(CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdIb4cgXd13D"
   },
   "source": [
    "### Text sequence generation\n",
    "\n",
    "In this section is prepared `text_sequence` which is input to the [text sequence analyzer](#text-sequence-analyzer) model.\n",
    "\n",
    "In this section is also prepared `anomalous_text_sequence`, which is a log message sequence meant to contain all possible permutation tuples, including anomalous tuples corresponding to `SKIPPED_LOG_SEQUENCE`. `anomalous_text_sequence` is used to reproduce an [anomaly](#anomaly-detection) and then show that there are no anomalies after the [corrective action](#corrective-action).\n",
    "\n",
    "Based on the output from the `get_log_message_type_summary_map` and `get_top_features` functions, `text_sequence` and `anomalous_text_sequence` are generated as texts of message summaries in *regex* format, one summary per line. The message summaries must be unique just as the message types are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7sY52Hud13D"
   },
   "outputs": [],
   "source": [
    "def get_log_message_type_summary_map(\n",
    "  model: Pipeline,\n",
    "  log_lines: list[str],\n",
    "  log_message_types: np.ndarray | None = None,\n",
    ") -> dict[int, str]:\n",
    "  \"\"\"Maps a cluster to regex representations of its messages.\n",
    "\n",
    "  A regex format is useful, because:\n",
    "    - it defines a unique summary for each learned log message from each\n",
    "      cluster, thus providing for a sequence of unique inputs to the\n",
    "      `text_sequence_analyzer` model;\n",
    "    - it provides for a means to determine whether a new log message is\n",
    "      actually a known one that is already learned by `text_sequence_analyzer`.\n",
    "\n",
    "  Args:\n",
    "    model: A `Scikit-learn` pipeline for log message clusterization.\n",
    "    log_lines: Log messages, one per line.\n",
    "    log_message_types: Optional, log message types predicted by `model`.\n",
    "      Can be set internally based on `model` and `log_lines`.\n",
    "\n",
    "  Returns:\n",
    "    A type-summary mapping for log messages, where the summary is a regex.\n",
    "  \"\"\"\n",
    "  np_log_lines = np.array(log_lines)\n",
    "  if log_message_types is None:\n",
    "    log_message_types = model.predict(np_log_lines)\n",
    "  message_type_summary_map: dict[int, str] = {}\n",
    "  for cluster, top_features in get_top_features(model).items():\n",
    "    messages = np_log_lines[np.where(log_message_types == cluster)]\n",
    "    fine_tokens = [msg.split() for msg in messages]\n",
    "    cluster_regexes: dict[int, list[str]] = {}\n",
    "    for msg_tokens in fine_tokens:\n",
    "      regex = create_regex(msg_tokens, top_features)\n",
    "      if cluster in cluster_regexes:\n",
    "        if regex not in cluster_regexes[cluster]:\n",
    "          cluster_regexes[cluster] += [regex]\n",
    "      else:\n",
    "        cluster_regexes[cluster] = [regex]\n",
    "    for msg in messages:\n",
    "      assert any(re.match(regex, msg) is not None\n",
    "                 for regex in cluster_regexes[cluster])\n",
    "    message_type_summary_map[cluster] = \"|\".join(\n",
    "      sorted(cluster_regexes[cluster])\n",
    "    )\n",
    "  return message_type_summary_map\n",
    "\n",
    "\n",
    "def create_regex(\n",
    "  msg_tokens: list[str],\n",
    "  top_features: set[str],\n",
    ") -> str:\n",
    "  sequence: list[str] = []\n",
    "  feature_sequence: list[str] = []\n",
    "  for token in msg_tokens:\n",
    "    if token in top_features:\n",
    "      feature_sequence += [token]\n",
    "    else:\n",
    "      if feature_sequence:\n",
    "        feature_sequence_str = r'\\s'.join(feature_sequence)\n",
    "        sequence += [feature_sequence_str, \".+\"]\n",
    "        feature_sequence = []\n",
    "      else:\n",
    "        if not sequence or sequence[-1] != \".+\":\n",
    "          sequence += [\".+\"]\n",
    "  if feature_sequence:\n",
    "    feature_sequence_str = r'\\s'.join(feature_sequence)\n",
    "    sequence += [feature_sequence_str]\n",
    "  sequence_str = r'\\s+'.join(sequence)\n",
    "  return f\"(^{sequence_str}$)\"\n",
    "\n",
    "\n",
    "def get_top_features(model: Pipeline) -> dict[int, set[str]]:\n",
    "  vectorizer = model.named_steps[\"countvectorizer\"]\n",
    "  all_features = vectorizer.get_feature_names_out()\n",
    "  top_feature_indices_per_cluster = np.argsort(\n",
    "    model.named_steps[\"truncatedsvd\"].inverse_transform(\n",
    "      model.named_steps[\"kmeans\"].cluster_centers_\n",
    "    )\n",
    "  )[:, ::-1]\n",
    "  top_features_per_cluster: dict[int, set[str]] = {}\n",
    "  for cluster, feature_indices in enumerate(top_feature_indices_per_cluster):\n",
    "    top_features: set[str] = set()\n",
    "    for feature in all_features[feature_indices]:\n",
    "      if model.score([feature]) <= -1:\n",
    "        break\n",
    "      top_features.add(feature)\n",
    "    if not top_features:\n",
    "      top_features = set(all_features[feature_indices][:3])\n",
    "    top_features_per_cluster[cluster] = top_features\n",
    "  return top_features_per_cluster\n",
    "\n",
    "\n",
    "if Path.exists(BOW_MODEL_PATH):\n",
    "  log_data_clusterizer = pickle.load(open(BOW_MODEL_PATH, \"rb\"))\n",
    "else:\n",
    "  pickle.dump(log_data_clusterizer, open(BOW_MODEL_PATH, \"wb\"))\n",
    "\n",
    "print(log_data_clusterizer)\n",
    "\n",
    "log_message_types = log_data_clusterizer.predict(log_lines)\n",
    "log_message_type_summary_map = get_log_message_type_summary_map(\n",
    "  log_data_clusterizer, log_lines, log_message_types\n",
    ")\n",
    "text_sequence = [\n",
    "  \"\\n\".join([log_message_type_summary_map[t]\n",
    "             for t in log_message_types])\n",
    "]\n",
    "anomalous_text_sequence = [\n",
    "  \"\\n\".join([\n",
    "    f\"{log_message_type_summary_map[p1]}\\n\"\n",
    "    f\"{log_message_type_summary_map[p2]}\"\n",
    "    for p1, p2 in itertools.permutations(np.unique(log_message_types), r=2)]\n",
    "  )\n",
    "]\n",
    "print(\"\\n\\nLog message type-summary mapping\")\n",
    "print(\"-\" * 80)\n",
    "log_message_type_summary_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74fQOngid13D"
   },
   "source": [
    "## Text sequence analyzer\n",
    "\n",
    "### Definitions\n",
    "The anomaly detection model is defined here. This model is named `text_sequence_analyzer` and is of type `TextSequenceAnalyzer`. To keep both preprocessing and training logic together in the subsequently saved model, an internal to the model class `Preprocessor` is defined.\n",
    "\n",
    "`text_sequence_analyzer` is an end-to-end sequence analyzer capable of whitespace-tokenizing raw textual inputs into sequences of token ids and predicting the next token id. The `detect_anomalies` method contains the anomaly detection logic - if a token id part of the input sequence is not predicted based on `NEXT_TOKEN_PROB_THRESHOLD`, `anomaly_detected_cb` is called with all data related to the anomaly.\n",
    "\n",
    "If a `text_sequence_analyzer.keras` model file in `Keras` format exists - the model is loaded from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV3s3x3Hd13E"
   },
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "class TextSequenceAnalyzer(Model):\n",
    "  \"\"\"RNN-based model that learns sequential data and detects anomalies.\n",
    "\n",
    "  This is an end-to-end model capable of tokenizing raw textual inputs\n",
    "  into sequences of token ids to predict the next token id.\n",
    "\n",
    "  To keep both preprocessing and training logic together in the subsequently\n",
    "  saved model, an internal to the model class `Preprocessor` is defined to\n",
    "  wrap a `tensorflow.keras.layers.TextVectorization` instance.\n",
    "\n",
    "  An input text sequence is learned by a combination of layers:\n",
    "  - a `Preprocessor` layer to build a *vocabulary* from the model's inputs;\n",
    "  - a `tensorflow.keras.layers.Embedding` layer with `EMBEDDING_DIM`\n",
    "    output vector size;\n",
    "  - a `tensorflow.keras.layers.GRU` sequence-to-sequence layer with `RNN_UNITS`\n",
    "    number of units;\n",
    "  - a `tensorflow.keras.layers.Dense` multinomial classification layer with\n",
    "    number of units matching the vocabulary size.\n",
    "\n",
    "  The `detect_anomalies` method contains the anomaly detection logic -\n",
    "  if a token id part of the input sequence is not predicted based on\n",
    "  `NEXT_TOKEN_PROB_THRESHOLD`, `anomaly_detected_cb` is called with all\n",
    "  data related to the anomaly.\n",
    "\n",
    "  This model is trained for multi-label predictions via `softmax` activation,\n",
    "  with `tensorflow.keras.losses.SparseCategoricalCrossentropy` used for loss\n",
    "  and `tensorflow.keras.optimizers.Adam` used for optimizer.\n",
    "  \"\"\"\n",
    "\n",
    "  @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "  class Preprocessor(Layer):\n",
    "    \"\"\"A preprocessing layer for `TextSequenceAnalyzer`.\n",
    "\n",
    "    This layer has two processing modes: for training and not for training.\n",
    "    The modes are specified as a parameter to the `preprocess` method, where is\n",
    "    the implementation for these modes.\n",
    "\n",
    "    In the for-training mode, the input text sequence is vectorized and split\n",
    "    into a shuffled and batched training set and a batched validation set\n",
    "    as per the `self._to_tf_dataset` method.\n",
    "\n",
    "    In the alternative mode, the input text sequence is only vectorized.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary: list[str]):\n",
    "      super().__init__()\n",
    "      self._text_vectorizer = TextVectorization(\n",
    "        standardize=None, split=tf.strings.split,\n",
    "        vocabulary=vocabulary,\n",
    "      )\n",
    "\n",
    "    @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "    def preprocess(\n",
    "      self,\n",
    "      text_sequence: list[str],\n",
    "      for_training: bool = False,\n",
    "      validation_split: int = VALIDATION_SPLIT,\n",
    "    ) -> tuple[Dataset, Dataset] | Tensor:\n",
    "      sequence: Tensor = self._text_vectorizer(text_sequence)[0]\n",
    "      if for_training:\n",
    "        if isinstance(validation_split, float):\n",
    "          validation_split = int(validation_split * len(sequence))\n",
    "        train_seq = sequence[:-validation_split]\n",
    "        train_ds = self._to_tf_dataset(train_seq, shuffle=True)\n",
    "        valid_seq = sequence[-validation_split:]\n",
    "        valid_ds = self._to_tf_dataset(valid_seq, shuffle=False)\n",
    "        return train_ds, valid_ds\n",
    "      else:\n",
    "        return sequence\n",
    "\n",
    "    @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "    def get_vocabulary(self) -> list[str]:\n",
    "      return self._text_vectorizer.get_vocabulary()  # type: ignore\n",
    "\n",
    "    @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "    @classmethod\n",
    "    def _to_tf_dataset(\n",
    "      cls,\n",
    "      sequence: Tensor,\n",
    "      shuffle: bool = False,\n",
    "      random_seed: int | None = None,\n",
    "    ) -> Dataset:\n",
    "      \"\"\"Converts integers to a dataset of shift-by-one series in tuples.\n",
    "\n",
    "      Args:\n",
    "        sequence: A tensor of integers.\n",
    "        shuffle: Whether to shuffle the input sequence.\n",
    "        seed: The random seed if `shuffle=True`.\n",
    "\n",
    "      Returns:\n",
    "        A dataset that is batched, can be shuffled, and is optimized\n",
    "        for access.\n",
    "      \"\"\"\n",
    "      dataset = Dataset.from_tensor_slices(sequence)\n",
    "      dataset = dataset.batch(SUBSEQUENCE_LENGTH+1, drop_remainder=True)\n",
    "      dataset = dataset.map(lambda seq: (seq[:-1], seq[1:]))\n",
    "      if shuffle:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE, seed=random_seed)\n",
    "      dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "      dataset = dataset.prefetch(AUTOTUNE)\n",
    "      return dataset\n",
    "\n",
    "\n",
    "  @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "  @classmethod\n",
    "  def create_preprocessor(\n",
    "    cls,\n",
    "    text_sequence: list[str],\n",
    "  ) -> \"TextSequenceAnalyzer.Preprocessor\":\n",
    "    text_vectorizer = TextVectorization(\n",
    "      standardize=None, split=tf.strings.split\n",
    "    )\n",
    "    text_vectorizer.adapt(text_sequence)\n",
    "    return TextSequenceAnalyzer.Preprocessor(\n",
    "      text_vectorizer.get_vocabulary()\n",
    "    )\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    preprocessor: \"TextSequenceAnalyzer.Preprocessor\",\n",
    "    embedding_dim: int,\n",
    "    rnn_units: int,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    vocab_size = len(preprocessor.get_vocabulary())\n",
    "    self._preprocessor = preprocessor\n",
    "    self._embedding_dim = embedding_dim\n",
    "    self._embedding = Embedding(\n",
    "      vocab_size, embedding_dim\n",
    "    )\n",
    "    self._rnn_units = rnn_units\n",
    "    self._gru = GRU(\n",
    "      rnn_units, return_sequences=True, return_state=True\n",
    "    )\n",
    "    self._dense = Dense(\n",
    "      vocab_size, activation=\"softmax\"\n",
    "    )\n",
    "\n",
    "  @override\n",
    "  def call(  # type: ignore\n",
    "    self,\n",
    "    inputs: Tensor,\n",
    "    training: bool = False,\n",
    "    mask: Tensor | None = None,\n",
    "    states: list[Any] | None = None,\n",
    "    return_state: bool = False,\n",
    "  ) -> tuple[Tensor, list[Any] | None] | Tensor:\n",
    "    x = inputs\n",
    "    x = self._embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self._gru.get_initial_state(x)\n",
    "    x, states = self._gru(x, initial_state=states, training=training)\n",
    "    x = self._dense(x, training=training)\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    return x\n",
    "\n",
    "  @register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "  def detect_anomalies(\n",
    "    self,\n",
    "    text: list[str],\n",
    "    anomaly_detected_cb: Callable[[list[str], list[int], int, list[int]],\n",
    "                                  None],\n",
    "  ) -> None:\n",
    "    \"\"\"Detects anomalies in input text and notifies about them via callback.\n",
    "\n",
    "    The input text is tokenized and the tokens are passed one-by-one\n",
    "    to the model to repeatedly obtain the next token prediction. In every\n",
    "    iteration the last token must be a valid next token as per the model,\n",
    "    i.e., with probability greater than `NEXT_TOKEN_PROB_THRESHOLD`,\n",
    "    otherwise anomaly is detected and notified via `anomaly_detected_cb`.\n",
    "\n",
    "    Args:\n",
    "      text: Input text that is to be tokenized by `self.preprocessor`.\n",
    "      anomaly_detected_cb: A callback to be called when anomaly is detected.\n",
    "    \"\"\"\n",
    "    sequence: list[int] = []\n",
    "    predictions: Tensor | None = None\n",
    "    states: list[Any] | None = None\n",
    "    predicted_token_ids: list[int] = []\n",
    "    token_ids = (self._preprocessor.preprocess(text)\n",
    "                 .numpy().ravel().tolist())\n",
    "    for token_id in token_ids:\n",
    "      sequence += [token_id]\n",
    "      if predictions is not None and token_id not in predicted_token_ids:\n",
    "        anomaly_detected_cb(self._preprocessor.get_vocabulary(), sequence,\n",
    "                            token_id, predicted_token_ids)\n",
    "      predictions, states = self.call(  # type: ignore\n",
    "        inputs=constant([[token_id]]), states=states, return_state=True\n",
    "      )\n",
    "      predictions = predictions[:, -1][0]\n",
    "      predicted_token_ids = tf.where(\n",
    "        predictions >= NEXT_TOKEN_PROB_THRESHOLD\n",
    "      ).numpy().ravel().tolist()\n",
    "\n",
    "  @property\n",
    "  def preprocessor(self) -> \"TextSequenceAnalyzer.Preprocessor\":\n",
    "    return self._preprocessor\n",
    "\n",
    "  @override\n",
    "  def get_config(self) -> dict[str, Any]:\n",
    "    config = super().get_config()\n",
    "    config.update({\n",
    "      \"preprocessor\": serialize_keras_object(\n",
    "        self._preprocessor\n",
    "      ),\n",
    "      \"embedding_dim\": self._embedding_dim,\n",
    "      \"rnn_units\": self._rnn_units,\n",
    "    })\n",
    "    return config\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "    cls,\n",
    "    config: dict[str, Any],\n",
    "    custom_objects=None,\n",
    "  ) -> \"TextSequenceAnalyzer\":\n",
    "    preprocessor = deserialize_keras_object(\n",
    "      config[\"preprocessor\"]\n",
    "    )\n",
    "    embedding_dim = deserialize_keras_object(\n",
    "      config[\"embedding_dim\"]\n",
    "    )\n",
    "    rnn_units = deserialize_keras_object(\n",
    "      config[\"rnn_units\"]\n",
    "    )\n",
    "    return cls(preprocessor, embedding_dim, rnn_units)\n",
    "\n",
    "\n",
    "if Path.exists(RNN_MODEL_PATH):\n",
    "  text_sequence_analyzer = load_model(RNN_MODEL_PATH)\n",
    "  assert(text_sequence_analyzer is not None)\n",
    "  preprocessor = text_sequence_analyzer.preprocessor\n",
    "else:\n",
    "  preprocessor = TextSequenceAnalyzer.create_preprocessor(text_sequence)\n",
    "  train_ds, valid_ds = preprocessor.preprocess(\n",
    "    text_sequence, for_training=True\n",
    "  )\n",
    "  text_sequence_analyzer = TextSequenceAnalyzer(\n",
    "    preprocessor=preprocessor,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_units=RNN_UNITS,\n",
    "  )\n",
    "  text_sequence_analyzer.build(input_shape=(None, 1))\n",
    "  text_sequence_analyzer.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                                 optimizer=\"adam\")\n",
    "  text_sequence_analyzer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2QU0sa6d13E"
   },
   "source": [
    "### Training\n",
    "\n",
    "The training process for `text_sequence_analyzer` is defined in the `train_sequence_analyzer` function. Training does not start in case of an already saved model, i.e., if a `text_sequence_analyzer.keras` file exists. If training does start, overfitting is prevented by usage of a `tensorflow.keras.callbacks.EarlyStopping` callback, `EARLY_STOP_PATIENCE` and `val_loss` as the monitored metric. The maximum number of training epochs is configured in `EPOCHS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlIzBKFNd13F"
   },
   "outputs": [],
   "source": [
    "def train_sequence_analyzer(\n",
    "  text_sequence_analyzer: TextSequenceAnalyzer,\n",
    "  train_ds: Dataset,\n",
    "  valid_ds: Dataset,\n",
    "  callbacks: list[Callback],\n",
    ") -> None:\n",
    "  history = text_sequence_analyzer.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "  )\n",
    "  plt.plot(history.history[\"val_loss\"], \"r--\",\n",
    "           label=\"val_loss\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.xlim([0, EPOCHS])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "early_stop_cb = EarlyStopping(\n",
    "  monitor=\"val_loss\",\n",
    "  patience=EARLY_STOP_PATIENCE,\n",
    "  restore_best_weights=True,\n",
    ")\n",
    "\n",
    "if not Path.exists(RNN_MODEL_PATH):\n",
    "  train_sequence_analyzer(\n",
    "    text_sequence_analyzer, train_ds, valid_ds,\n",
    "    callbacks=[early_stop_cb],\n",
    "  )\n",
    "  text_sequence_analyzer.save(RNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT6dNFppd13F"
   },
   "source": [
    "### Anomaly detection\n",
    "\n",
    "In this section the `test_for_anomalies` function demonstrates anomaly detection.\n",
    "\n",
    "It must be noted that there can be *false positives* in text generated by `generate_log`, this is when `text_sequence_analyzer` is under-trained on otherwise valid sequences. Such *false positives* can also be detected by setting \"high\" `NEXT_TOKEN_PROB_THRESHOLD` in combination with \"low\" `EPOCHS`, again causing `text_sequence_analyzer` to have a high bias on `train_ds` and mispredict the next token. Setting too small values for `LOG_SIZE`, `SUBSEQUENCE_LENGTH` or `EARLY_STOP_PATIENCE` can also lead to under-trained model and *false positives*. On the other hand, an overfitting model can also have problems with *false positives* because it can assign high probabilities due to focusing too much on certain log messages.\n",
    "\n",
    "Interestingly, in the above-mentioned cases anomaly detection can be used as an indicator for an under-trained model.\n",
    "\n",
    "Also important is the regular case of false positives - new to the model *normal* data which simply has to be learned. This case is demonstrated when the `detect_anomalies` method is called with `anomalous_text_sequence` which is meant to contain `SKIPPED_LOG_SEQUENCE`-based subsequences that have not been learned by `text_sequence_analyzer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO4muWmgd13F"
   },
   "outputs": [],
   "source": [
    "def on_anomaly_detected(\n",
    "  vocabulary: list[str],\n",
    "  sequence: list[int],\n",
    "  token_id: int,\n",
    "  predicted_token_ids: list[int],\n",
    ") -> None:\n",
    "  sequence_str = SEPARATOR.join([\n",
    "    f\"{i}.{vocabulary[n_token_id]}\"\n",
    "    for i, n_token_id in enumerate(sequence, start=1)\n",
    "  ])\n",
    "  token_id_str = vocabulary[token_id]\n",
    "  predicted_token_ids_str = SEPARATOR.join([\n",
    "    vocabulary[p_token_id] for p_token_id in predicted_token_ids\n",
    "  ])\n",
    "  print(\"ANOMALY DETECTED\")\n",
    "  print(\"-\" * 80)\n",
    "  print(\"SEQUENCE:\")\n",
    "  text_wrapper = TextWrapper()\n",
    "  sequence_lines = text_wrapper.wrap(sequence_str)\n",
    "  for line in sequence_lines:\n",
    "    print(line)\n",
    "  print(\"-\" * 80)\n",
    "  print(f\"{token_id_str}\\nNOT IN\")\n",
    "  predicted_token_ids_lines = text_wrapper.wrap(\n",
    "    predicted_token_ids_str\n",
    "  )\n",
    "  for line in predicted_token_ids_lines:\n",
    "    print(line)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def test_for_anomalies(\n",
    "  text_sequence_analyzer: TextSequenceAnalyzer,\n",
    "  log_data_clusterizer: Pipeline,\n",
    "  type_summary_map: dict[int, str],\n",
    "  anomaly_detected_cb: Callable[[list[str], list[int], int, list[int]],\n",
    "                                None],\n",
    ") -> None:\n",
    "  log_text = generate_log(log_gen, SUBSEQUENCE_LENGTH)\n",
    "  log_lines = log_text[0].splitlines()\n",
    "  log_message_types = log_data_clusterizer.predict(log_lines)\n",
    "  text_sequence = [\n",
    "    \"\\n\".join([type_summary_map[t]\n",
    "               for t in log_message_types])\n",
    "  ]\n",
    "  # There should be no anomalies with generate_log\n",
    "  # unless the model is undertrained.\n",
    "  text_sequence_analyzer.detect_anomalies(\n",
    "    text_sequence,\n",
    "    anomaly_detected_cb,\n",
    "  )\n",
    "  text_sequence_analyzer.detect_anomalies(\n",
    "    anomalous_text_sequence,\n",
    "    anomaly_detected_cb,\n",
    "  )\n",
    "\n",
    "\n",
    "test_for_anomalies(\n",
    "  text_sequence_analyzer, log_data_clusterizer,\n",
    "  log_message_type_summary_map, on_anomaly_detected,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuU5013rd13G"
   },
   "source": [
    "### Corrective action\n",
    "\n",
    "`SKIPPED_LOG_SEQUENCE` is made invalid after set to *(-1, -1)*, so that new log generation should include any previously skipped sequence and thus `text_sequence_analyzer` can learn that such a sequence is not anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX4J3OAJd13G"
   },
   "outputs": [],
   "source": [
    "SKIPPED_LOG_SEQUENCE = (-1, -1)\n",
    "new_log_text = generate_log(log_gen)\n",
    "new_log_lines = new_log_text[0].splitlines()\n",
    "new_log_message_types = log_data_clusterizer.predict(\n",
    "  new_log_lines\n",
    ")\n",
    "new_text_sequence = [\n",
    "  \"\\n\".join([log_message_type_summary_map[t]\n",
    "             for t in new_log_message_types])\n",
    "]\n",
    "new_train_ds, new_valid_ds = preprocessor.preprocess(\n",
    "  new_text_sequence, for_training=True\n",
    ")\n",
    "train_sequence_analyzer(\n",
    "  text_sequence_analyzer, new_train_ds, new_valid_ds,\n",
    "  callbacks=[early_stop_cb],\n",
    ")\n",
    "test_for_anomalies(\n",
    "  text_sequence_analyzer, log_data_clusterizer,\n",
    "  log_message_type_summary_map, on_anomaly_detected,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaQwcK11d13G"
   },
   "source": [
    "## New log data\n",
    "\n",
    "In this section is demonstrated how new log data can be interpreted based on `log_message_type_summary_map`. Data that does not fit any known cluster can either be included for re-training of the `log_data_clusterizer` and `text_sequence_analyzer` models, or can simply be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAWPoH97d13G"
   },
   "outputs": [],
   "source": [
    "test_log_message = new_log_lines[0]\n",
    "test_log_message_matched = False\n",
    "for cluster, pattern in log_message_type_summary_map.items():\n",
    "  if re.match(pattern, test_log_message) is not None:\n",
    "    test_log_message_matched = True\n",
    "    print(f\"'{test_log_message}' is a known log message from \"\n",
    "          f\"cluster {cluster}.\")\n",
    "    break\n",
    "if not test_log_message_matched:\n",
    "  print(f\"'{test_log_message}' is not a known log message.\")\n",
    "log_message_type_summary_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP2JQa2nd13G"
   },
   "source": [
    "## Reset global state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ_EmaLMd13G"
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuL2VIymd13G"
   },
   "source": [
    "## References\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### APA style for references\n",
    "American Psychological Association. (2022). Creating an APA Style reference list guide. https://apastyle.apa.org/instructional-aids/creating-reference-list.pdf\n",
    "\n",
    "American Psychological Association. (2024). APA Style common reference examples guide. https://apastyle.apa.org/instructional-aids/reference-examples.pdf\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### A survey on the application of deep learning for anomaly detection in logs\n",
    "Himler, P., Landauer, M., Skopik, F., & Wurzenberger, M. (2024). Anomaly detection in log-event sequences: A federated deep learning approach and open challenges. *Machine Learning with Applications*, *16*, 100554. https://doi.org/10.1016/j.mlwa.2024.100554\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Deeplog: Anomaly detection and diagnosis from system logs through deep learning\n",
    "Du, M., Li, F., Zheng, G., & Srikumar, V. (2017, October). Deeplog: Anomaly detection and diagnosis from system logs through deep learning. *In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security* (pp. 1285-1298). https://doi.org/10.1145/3133956.3134015\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Datasets\n",
    "##### Comprehensive Network Logs Dataset for Multi-Device Analysis\n",
    "Salman, M., & Hasan, R. (2024). Comprehensive Network Logs Dataset for Multi-Device Analysis (Version v1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.10492770\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning methods\n",
    "##### Singular value decomposition\n",
    "- [Singular value decomposition - Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "##### K-means clustering\n",
    "- [k-means clustering - Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "##### Recurrent neural network\n",
    "- [Recurrent neural network - Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning models\n",
    "##### Bag-of-words\n",
    "- [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### GitHub repos\n",
    "- [GitHub - ageron/handson-ml3: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.](https://github.com/ageron/handson-ml3)\n",
    "  - [handson-ml3/16_nlp_with_rnns_and_attention.ipynb at main · ageron/handson-ml3 · GitHub](https://github.com/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Guides and tutorials\n",
    "- [Clustering text documents using k-means — scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#k-means-clustering-on-text-features)\n",
    "- [Save, serialize, and export models  |  TensorFlow Core](https://www.tensorflow.org/guide/keras/serialization_and_saving#custom_objects)\n",
    "- [Text generation with an RNN  |  TensorFlow](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "- [Working with preprocessing layers  |  TensorFlow Core](https://www.tensorflow.org/guide/keras/preprocessing_layers)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Libraries\n",
    "##### Keras\n",
    "Chollet, F., & others. (2015). Keras. https://keras.io\n",
    "- [Getting started with Keras](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility)\n",
    "##### Matplotlib\n",
    "Hunter, J. D. (May-June 2007). Matplotlib: A 2D Graphics Environment. *Computing in Science & Engineering*, *9*(3), 90-95. https://doi.org/10.1109/MCSE.2007.55\n",
    "- [Quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html)\n",
    "##### Numpy\n",
    "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., ... Oliphant, T. E. (2020). Array programming with NumPy. *Nature*, *585*, 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- [What is NumPy?](https://numpy.org/doc/2.2/user/whatisnumpy.html)\n",
    "##### Pandas\n",
    "The pandas development team. pandas-dev/pandas: Pandas [Computer software]. https://doi.org/10.5281/zenodo.3509134\n",
    "- [Getting started — pandas](https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas)\n",
    "##### Scikit-learn\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, *12*, 2825–2830. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
    "- [Getting Started — scikit-learn](https://scikit-learn.org/stable/getting_started.html)\n",
    "##### TensorFlow\n",
    "Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jozefowicz, R., Jia, Y., Kaiser, L., Kudlur, M., ... Zheng, X. (2015). TensorFlow, Large-scale machine learning on heterogeneous systems [Computer software]. https://doi.org/10.5281/zenodo.4724125\n",
    "- [Introduction to TensorFlow](https://www.tensorflow.org/learn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
