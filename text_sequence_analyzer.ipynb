{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oomK1B-zHge3"
   },
   "source": [
    "\n",
    "# A generic hands-on deep learning method for anomaly detection in sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8HHmBCfmKDh"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzGuTzTBvj2R"
   },
   "source": [
    "### Motivation\n",
    "\n",
    "This notebook demonstrates how unexpected subsequences (anomalies) can be found in sequential data. The example implementation uses [software logs](https://en.m.wikipedia.org/wiki/Logging_(computing)).\n",
    "\n",
    "Logs can be very helpful in understanding the behavior of a new system, application or environment we have only recently started to work with. The latter situation involves some incremental learning process - from human and ... possibly machine standpoint. That is, machine learning (ML) can be utilized as a powerful tool in log analysis.\n",
    "\n",
    "Log messages, in general, are very specific to the activities being logged and thus can contain numeric data, so anomaly detection methods based on [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) or [bag-of-words (BoW)](https://en.wikipedia.org/wiki/Bag-of-words_model) can be applied only to a limited extent. For the purpose of genericity, the most important *component* of a log to be analyzed by this notebook's method is the *sequence* of log messages.\n",
    "\n",
    "Focusing on the log message *sequences*, there are 2 main tasks involved in preparing log data for ML-aided analysis:\n",
    "1. cleaning, filtering and sorting - to obtain only the log messages of interest;\n",
    "2. classification or clusterization of the selected log messages into messages types to be processed as a sequence.\n",
    "\n",
    "Both task 1 and task 2, but especially task 2, can be accomplished via ML.\n",
    "\n",
    "Task 1 is important in filtering out only the relevant log information, especially when the log is huge in size (e.g., gigabytes). Regular expressions or even ML can be utilized for solving this task.\n",
    "\n",
    "Task 2 is based on task 1, and for solving this task some ML text classification or clustering algorithm based on NLP or BoW can be utilized, as compared to using the pure regular expressions approach.\n",
    "\n",
    "After task 1 and task 2 are completed, a log is converted to a sequence of *message types* rather than messages.\n",
    "\n",
    "|  Log             |\n",
    "|------------------|\n",
    "|  Message type 1  |\n",
    "|  ...             |\n",
    "|  Message type n  |\n",
    "\n",
    "\n",
    "However, mentioning *message type* is just for clarification regarding how the method works, so in this notebook a log entry is just called *log message*.\n",
    "\n",
    "**For reasons of simplification and conciseness, in this notebook neither task 1 nor task 2 is demonstrated and both tasks are considered as already completed.** Thus, in the [Bluetooth log generator](#bluetooth-log-generator) section is implemented a log generator for simplified Bluetooth communication state changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zsz1xwimKDi"
   },
   "source": [
    "### Method\n",
    "\n",
    "As per [A survey on the application of deep learning for anomaly detection in logs](#a-survey-on-the-application-of-deep-learning-for-anomaly-detection-in-logs), there exist various sequence-based anomaly detection solutions. **In particular, this notebook demonstrates a hands-on, generic in its simplicity, method for finding anomalies in a potentially large amount of logs by utilizing a [Recurrent Neural Network (RNN)](https://en.m.wikipedia.org/wiki/Recurrent_neural_network).**\n",
    "\n",
    "The selected method is a [semi-supervised](https://en.wikipedia.org/wiki/Weak_supervision) one based on the following step-by-step algorithm:\n",
    "1. training process to learn possible log message sequences in already available normal log data (the supervised part);\n",
    "2. next log message predictions on new log data;\n",
    "3. [optional] if in step 2 a next log message cannot be predicted based on what is learned in step 1, a human intervention is needed to determine whether this is an anomaly or just a new log message to be learned (the unsupervised part);\n",
    "4. [optional] if in step 3 are found new normal log messages, go to step 1, else: go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsX2HD9-zhdP"
   },
   "source": [
    "#### Case study: anomaly detection in logged communication with remote devices via Bluetooth\n",
    "\n",
    "In this notebook is demonstrated a method to solve an anomaly detection task: a Bluetooth communication is simplified into logged state changes among states `BT_STATE_NAMES` with each state change following one of the allowed transitions in `BT_STATE_TRANSITIONS`, and the task is to detect unexpected next state as part of a sequence of state changes.\n",
    "\n",
    "Though not much needed to understand this case study as `state-to-state transitions` are the essence and [state machines](https://en.wikipedia.org/wiki/Finite-state_machine) are a general topic, some clarifications on the used Bluetooth states-related terminology would still be helpful.\n",
    "\n",
    "A remote Bluetooth device is `unknown` if (obviously) not known to the host system, `pairing` is needed to make a remote device known to the host system and upon successful pairing the remote device becomes `connected` to the host while upon failed pairing the remote device becomes `unknown` again. To utilize a Bluetooth connection, some Bluetooth application protocol needs to be run for this connection or the remote device soon becomes `disconnected`. One standard Bluetooth application protocol (called profile) is `HF` (hands-free, to support a call app). Hence, `connected_hf` state allows for subsequent `call_app` state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4n-wAwUAmKDj"
   },
   "outputs": [],
   "source": [
    "BT_STATE_NAMES = [\n",
    "  \"unknown\", \"discovery\", \"pairing\",\n",
    "  \"disconnected\", \"connected\",\n",
    "  \"connected_hf\", \"call_app\",\n",
    "]\n",
    "\n",
    "\n",
    "BT_STATE_TRANSITIONS = {\n",
    "  \"unknown\" : [\"discovery\"],\n",
    "  \"discovery\" : [\"unknown\", \"pairing\"],\n",
    "  \"pairing\" : [\"connected\", \"unknown\"],\n",
    "  \"disconnected\" : [\"connected\", \"unknown\"],\n",
    "  \"connected\" : [\"disconnected\", \"connected_hf\"],\n",
    "  \"connected_hf\" : [\"disconnected\", \"connected\", \"call_app\"],\n",
    "  \"call_app\" : [\"connected\", \"connected_hf\"],  # \"disconnected\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwaCON-JmKDl"
   },
   "source": [
    "## System setup\n",
    "\n",
    "[TensorFlow](#tensorflow) and [Keras 2](#keras) are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDM27gQHDWBl"
   },
   "outputs": [],
   "source": [
    "import collections.abc as abc\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import random\n",
    "import textwrap\n",
    "import typing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.keras.backend as tf_backend\n",
    "import tensorflow.keras.callbacks as tf_callbacks\n",
    "import tensorflow.keras.layers as tf_layers\n",
    "import tensorflow.keras.models as tf_models\n",
    "import tensorflow.keras.saving as tf_saving\n",
    "import tensorflow.keras.utils as tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3E-pvqxEio5"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oCSNSXFuEN2u"
   },
   "outputs": [],
   "source": [
    "# Common\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data\n",
    "BT_DEVICE_COUNT = 3\n",
    "BT_LOG_SIZE = 100_000\n",
    "SEQUENCE_LENGTH = 100\n",
    "VALIDATION_SPLIT = 10_000\n",
    "\n",
    "# Output formatting\n",
    "SEPARATOR = \" \" * 3\n",
    "\n",
    "# Model\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10_000\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "EPOCHS = 1_000\n",
    "\n",
    "# Testing\n",
    "NEXT_TOKEN_PROB_THRESHOLD = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5oEd-WUEnwC"
   },
   "source": [
    "## Bluetooth log generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SokNig5tmKDn"
   },
   "source": [
    "### Definitions\n",
    "\n",
    "The Bluetooth communication log is generated for `BT_DEVICE_COUNT` devices by randomly following the allowed state transitions as per `BT_STATE_NAMES` and `BT_STATE_TRANSITIONS`. The last states of the `BT_DEVICE_COUNT` devices can be obtained via the `BtLogGenerator.device_states` method.\n",
    "\n",
    "Additionally, here are defined 2 helper methods:\n",
    "- `new_bt_log_gen` is used to construct a new log generator instance;\n",
    "- `generate_bt_log` is used to generate a log of size `log_size`, based on the initial states obtained via `BtLogGenerator.device_states`.\n",
    "\n",
    "The `bt_log_gen` log generator instance is global to the notebook and is used for incremental training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BwarQMMrEDKz"
   },
   "outputs": [],
   "source": [
    "class BtDevice:\n",
    "  \"\"\"Bluetooth device.\n",
    "\n",
    "  Attributes:\n",
    "    id: Identifier of this device.\n",
    "    state_index: A valid index within BT_STATE_NAMES.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               id: int,\n",
    "               state_index: int = BT_STATE_NAMES.index(\"unknown\")):\n",
    "    self.id = id\n",
    "    self.state_index = state_index\n",
    "\n",
    "\n",
    "class BtLogGenerator:\n",
    "  \"\"\"Bluetooth log generator implemented as a callable.\n",
    "\n",
    "  The log is generated for BT_DEVICE_COUNT devices by randomly following\n",
    "  the allowed state transitions as per BT_STATE_NAMES and BT_STATE_TRANSITIONS.\n",
    "  The last states of the BT_DEVICE_COUNT devices can be obtained via the\n",
    "  BtLogGenerator.device_states method.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               device_list: list[BtDevice]):\n",
    "    assert device_list\n",
    "    self._device_list = device_list\n",
    "\n",
    "  def device_states(self) -> list[str]:\n",
    "    return [self._state_repr(device.id, BT_STATE_NAMES[device.state_index])\n",
    "            for device in self._device_list]\n",
    "\n",
    "  def __call__(self) -> abc.Generator[str]:\n",
    "    device_id = random.randint(0, len(self._device_list) - 1)\n",
    "    device = self._device_list[device_id]\n",
    "    new_state_index, new_state_name = self._pick_new_state(device.state_index)\n",
    "    device.state_index = new_state_index\n",
    "    yield self._state_repr(device_id, new_state_name)\n",
    "\n",
    "  def _state_repr(self,\n",
    "                  device_id: int,\n",
    "                  state_name: str) -> str:\n",
    "    return f\"{device_id}-{state_name}\"\n",
    "\n",
    "  def _pick_new_state(self,\n",
    "                      state_index: int) -> tuple[int, str]:\n",
    "    state_name = BT_STATE_NAMES[state_index]\n",
    "    new_states = BT_STATE_TRANSITIONS[state_name]\n",
    "    new_state_name = random.choice(new_states)\n",
    "    return BT_STATE_NAMES.index(new_state_name), new_state_name\n",
    "\n",
    "\n",
    "def new_bt_log_gen() -> BtLogGenerator:\n",
    "  return BtLogGenerator(device_list=[\n",
    "    BtDevice(i) for i in range(BT_DEVICE_COUNT)\n",
    "  ])\n",
    "\n",
    "\n",
    "def generate_bt_log(bt_log_gen: BtLogGenerator,\n",
    "                    log_size: int = BT_LOG_SIZE) -> list[str]:\n",
    "  device_states = bt_log_gen.device_states()\n",
    "  return [\n",
    "    \"\\n\".join(device_states + [next(bt_log_gen()) for _ in range(log_size)])\n",
    "  ]\n",
    "\n",
    "\n",
    "tf_utils.set_random_seed(RANDOM_SEED)\n",
    "bt_log_gen = new_bt_log_gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyQGETb7mKDn"
   },
   "source": [
    "### New log\n",
    "\n",
    "`bt_log_data` contains newly-sampled BT communication log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CSmND0VcmKDo"
   },
   "outputs": [],
   "source": [
    "bt_log_data = generate_bt_log(bt_log_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM5VBPgFHM-J"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The unique log messages are first converted to numeric tokens by the `tensorflow.keras.layers.TextVectorization` layer named `text_vectorizer`, then are split into training and validation data based on `VALIDATION_SPLIT`, and finally are transformed into a `tensorflow.data.Dataset` dataset. The dataset contains `BATCH_SIZE` batches of `SEQUENCE_LENGTH` shift-by-one series in tuples, with each tuple representing an old-new state transition. The training dataset `train_ds` is shuffled over `BUFFER_SIZE` numeric tokens; no need to do shuffling for the validation set `valid_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYQ4amP6HMND"
   },
   "outputs": [],
   "source": [
    "def to_dataset(sequence: abc.Iterable[int],\n",
    "               shuffle: bool = False,\n",
    "               random_seed: int | None = None) -> tf_data.Dataset:\n",
    "  \"\"\"Converts integers to a dataset of shift-by-one series in tuples.\n",
    "\n",
    "  Args:\n",
    "    sequence: An iterable of integers.\n",
    "    shuffle: Whether to shuffle the input sequence.\n",
    "    seed: The random seed in case shuffling is enabled.\n",
    "\n",
    "  Returns:\n",
    "    A dataset that is batched, can be shuffled, and is optimized for access.\n",
    "  \"\"\"\n",
    "  dataset = tf_data.Dataset.from_tensor_slices(sequence)\n",
    "  dataset = dataset.batch(SEQUENCE_LENGTH+1, drop_remainder=True)\n",
    "  dataset = dataset.map(lambda seq: (seq[:-1], seq[1:]))\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE, seed=random_seed)\n",
    "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "  dataset = dataset.prefetch(tf_data.AUTOTUNE)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "@tf_saving.register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "def tokenize(x: str) -> tf.RaggedTensor:\n",
    "  return tf.strings.split(x, sep=\"\\n\")\n",
    "\n",
    "\n",
    "text_vectorizer = tf_layers.TextVectorization(\n",
    "  standardize=None, split=tokenize\n",
    ")\n",
    "text_vectorizer.adapt(bt_log_data)\n",
    "bt_log_sequence = text_vectorizer(bt_log_data)[0]\n",
    "\n",
    "train_ds = to_dataset(bt_log_sequence[:-VALIDATION_SPLIT],\n",
    "                      shuffle=True, random_seed=RANDOM_SEED)\n",
    "valid_ds = to_dataset(bt_log_sequence[-VALIDATION_SPLIT:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrfFkXDRua-E"
   },
   "source": [
    "## Sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll5MYz6SlJZn"
   },
   "source": [
    "### Definitions\n",
    "\n",
    "The preprocessed by `text_vectorizer` log sequences are learned by a basic RNN model named `sequence_model`, which consists of:\n",
    "- a `tensorflow.keras.layers.Embedding` layer with `EMBEDDING_DIM` output vector size;\n",
    "- a `tensorflow.keras.layers.GRU` sequence-to-sequence layer with `RNN_UNITS` number of units;\n",
    "- a `tensorflow.keras.layers.Dense` multinomial classification layer with number of units matching the `text_vectorizer`'s vocabulary size.\n",
    "\n",
    "`sequence_model` is trained for multi-label predictions via `softmax` activation, with `tensorflow.keras.losses.SparseCategoricalCrossentropy` used for loss and `tensorflow.keras.optimizers.Adam` used for optimizer.\n",
    "\n",
    "In this notebook no hyperparameter tuning is performed because the main purpose of the notebook is to demonstrate a working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp-5AyI4ua-F"
   },
   "outputs": [],
   "source": [
    "@tf_saving.register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "class SequenceModel(tf_models.Model):\n",
    "  \"\"\"RNN-based model that learns sequential data.\n",
    "\n",
    "  Attributes:\n",
    "    embedding: A Keras Embedding layer which creates vectorized\n",
    "      representations for tokenized by TextVectorizer text.\n",
    "    gru: A Keras Gated Recurrent Unit (GRU) layer that learns sequences based\n",
    "      on the embedding layers's output vectors.\n",
    "    dense: A Keras Dense layer which performs multi-label next-token\n",
    "      classification as the model's output.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               embedding_dim: int,\n",
    "               rnn_units: int,\n",
    "               vocab_size: int):\n",
    "    super().__init__()\n",
    "    self.embedding = tf_layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf_layers.GRU(\n",
    "      rnn_units, return_sequences=True, return_state=True\n",
    "    )\n",
    "    self.dense = tf_layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "  def call(self,\n",
    "           inputs: tf.Tensor,\n",
    "           states: tf.Tensor | None = None,\n",
    "           return_state: bool = False,\n",
    "           training: bool = False) -> tf.Tensor:\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    return x\n",
    "\n",
    "\n",
    "sequence_model = SequenceModel(\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=RNN_UNITS,\n",
    "  vocab_size=text_vectorizer.vocabulary_size(),\n",
    ")\n",
    "sequence_model.build(input_shape=(None, 1))\n",
    "sequence_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\")\n",
    "sequence_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzggrXr5ua-F"
   },
   "source": [
    "### Training\n",
    "\n",
    "The training process for `sequence_model` is defined in the `train` function. Training does not start in case of an already saved model, i.e., if a `text_sequence_analyzer.keras` file exists. If training does start, overfitting is prevented by usage of a `tensorflow.keras.callbacks.EarlyStopping` callback, with patience `EARLY_STOP_PATIENCE` and `val_loss` as the monitored metric. The maximum number of training epochs is configured in `EPOCHS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qUwje7iWua-F"
   },
   "outputs": [],
   "source": [
    "def train(sequence_model: tf_models.Model,\n",
    "          train_ds: tf_data.Dataset,\n",
    "          valid_ds: tf_data.Dataset,\n",
    "          callbacks: list[abc.Callable]):\n",
    "  history = sequence_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    "  )\n",
    "  plt.plot(history.history[\"val_loss\"], \"r--\", label=\"val_loss\")\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.xlim([0, EPOCHS])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "early_stop_cb = tf_callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\",\n",
    "  patience=EARLY_STOP_PATIENCE,\n",
    "  restore_best_weights=True\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"text_sequence_analyzer.keras\"):\n",
    "  train(sequence_model, train_ds, valid_ds,\n",
    "        callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI0CCsMd1AWA"
   },
   "source": [
    "## Text sequence analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTYgnIORmKDp"
   },
   "source": [
    "### Definitions\n",
    "\n",
    "The anomaly detection model is defined here. This model is named `text_sequence_analyzer` and consists of the previously defined in the notebook `text_vectorizer` and `sequence_model`. Thus, `text_sequence_analyzer` is an end-to-end sequence analyzer capable of tokenizing raw textual inputs into sequences of token ids and predicting the next token id. The `detect_anomalies` method contains the anomaly detection logic - if a token id part of the input sequence is not predicted, `anomaly_detected_cb` is called with all data related to the anomaly.\n",
    "\n",
    "If a `text_sequence_analyzer.keras` model file in Keras format exists - the model is loaded from this file, otherwise the model is saved to `text_sequence_analyzer.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ceM3eORj1Bv1"
   },
   "outputs": [],
   "source": [
    "@tf_saving.register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "class TextSequenceAnalyzer(tf_models.Model):\n",
    "  \"\"\"Detects anomalies in text data represented as a sequence.\n",
    "\n",
    "  This is an end-to-end model capable of tokenizing raw textual inputs\n",
    "  into sequences of token ids to predict the next token id.\n",
    "  The detect_anomalies method contains the anomaly detection logic -\n",
    "  if a token id part of the input sequence is not predicted,\n",
    "  anomaly_detected_cb is called with all data related to the anomaly.\n",
    "\n",
    "\n",
    "  Attributes:\n",
    "    text_vectorizer: A Keras TextVectorization layer which tokenizes\n",
    "      raw textual data into a sequence of integer tokens.\n",
    "    sequence_model: A SequenceModel which predicts next token based on\n",
    "      the text_vectorizer's output sequence.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               text_vectorizer: tf_layers.TextVectorization,\n",
    "               sequence_model: tf_models.Model):\n",
    "    super().__init__()\n",
    "    self.text_vectorizer = text_vectorizer\n",
    "    self.sequence_model = sequence_model\n",
    "\n",
    "  @tf_saving.register_keras_serializable(package=\"text_sequence_analysis\")\n",
    "  def detect_anomalies(self,\n",
    "                       text: str,\n",
    "                       anomaly_detected_cb: abc.Callable):\n",
    "    \"\"\"Detects anomalies in input text and notifies about them via callback.\n",
    "\n",
    "    The input text is tokenized and the tokens are passed one-by-one\n",
    "    to the model to repeatedly obtain the next token prediction. In every\n",
    "    iteration the last token must be a valid next token as per the model,\n",
    "    i.e., with probability greater than NEXT_TOKEN_PROB_THRESHOLD,\n",
    "    otherwise anomaly is detected and notified via anomaly_detected_cb.\n",
    "\n",
    "    Args:\n",
    "      text: Input text that is to be tokenized by self.text_vectorizer.\n",
    "      anomaly_detected_cb: A callback to be called when anomaly is detected.\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    predictions = None\n",
    "    states = None\n",
    "    predicted_token_ids = []\n",
    "    token_ids = self.text_vectorizer([text]).numpy().ravel().tolist()\n",
    "    for token_id in token_ids:\n",
    "      sequence += [token_id]\n",
    "      if predictions is not None and token_id not in predicted_token_ids:\n",
    "        anomaly_detected_cb(sequence, token_id, predicted_token_ids)\n",
    "      predictions, states = self.sequence_model(\n",
    "        inputs=tf.constant([[token_id]]), states=states, return_state=True\n",
    "      )\n",
    "      predictions = predictions[:, -1][0]\n",
    "      predicted_token_ids = tf.where(\n",
    "        predictions >= NEXT_TOKEN_PROB_THRESHOLD\n",
    "      ).numpy().ravel().tolist()\n",
    "\n",
    "  def get_config(self) -> dict[str, typing.Any]:\n",
    "    config = super().get_config()\n",
    "    config.update({\n",
    "      \"text_vectorizer\" : tf_saving.serialize_keras_object(\n",
    "        self.text_vectorizer\n",
    "      ),\n",
    "      \"sequence_model\" : tf_saving.serialize_keras_object(\n",
    "        self.sequence_model\n",
    "      ),\n",
    "    })\n",
    "    return config\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls,\n",
    "                  config: dict[str, typing.Any]) -> \"TextSequenceAnalyzer\":\n",
    "    text_vectorizer = tf_saving.deserialize_keras_object(\n",
    "      config[\"text_vectorizer\"]\n",
    "    )\n",
    "    sequence_model = tf_saving.deserialize_keras_object(\n",
    "      config[\"sequence_model\"]\n",
    "    )\n",
    "    return cls(text_vectorizer, sequence_model)\n",
    "\n",
    "\n",
    "if os.path.exists(\"text_sequence_analyzer.keras\"):\n",
    "  text_sequence_analyzer = tf_models.load_model(\n",
    "    \"text_sequence_analyzer.keras\"\n",
    "  )\n",
    "  sequence_model = text_sequence_analyzer.sequence_model\n",
    "else:\n",
    "  text_sequence_analyzer = TextSequenceAnalyzer(\n",
    "    text_vectorizer, sequence_model\n",
    "  )\n",
    "  text_sequence_analyzer.save(\"text_sequence_analyzer.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYawT6femKDq"
   },
   "source": [
    "### Anomaly detection\n",
    "\n",
    "In this section the `test_for_anomalies` function demonstrates anomaly detection.\n",
    "\n",
    "It must be noted that there can be *false positives* in text generated by `generate_bt_log`, this is when `sequence_model` is under-trained on otherwise valid Bluetooth state sequences. Such *false positives* can also be detected by setting \"high\" `NEXT_TOKEN_PROB_THRESHOLD` in combination with \"low\" `EPOCHS`, again causing `sequence_model` to have a high bias on `train_ds` and mispredict the next token. Setting too small values for `BT_LOG_SIZE`, `SEQUENCE` or `EARLY_STOP_PATIENCE` can also lead to under-trained model and *false positives*. On the other hand, an overfitting model can also have problems with *false positives* because it can assign high probabilities due to focusing too much on certain log messages.\n",
    "\n",
    "Interestingly, in the above-mentioned cases anomaly detection can be used as an indicator for an under-trained model.\n",
    "\n",
    "Also important is the regular case of *false positives* - new normal data which simply has to be learned. This case is demonstrated when `detect_anomalies` is called with `bt_log_data_call_app_disconnected` which contains a transition not part of `BT_STATE_TRANSITIONS`: `call_app -> disconnected`.\n",
    "\n",
    "An actual case of anomaly is demonstrated when `detect_anomalies` is called with `bt_log_data_call_app_discovery` which contains an invalid transition :  `call_app -> discovery`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV8cKorSmKDq"
   },
   "outputs": [],
   "source": [
    "def on_anomaly_detected(sequence: list[int],\n",
    "                        token_id: int,\n",
    "                        predicted_token_ids: list[int]):\n",
    "  vocabulary = text_vectorizer.get_vocabulary()\n",
    "  sequence_str = SEPARATOR.join([\n",
    "    vocabulary[n_token_id] for n_token_id in sequence\n",
    "  ])\n",
    "  token_id_str = vocabulary[token_id]\n",
    "  predicted_token_ids_str = SEPARATOR.join([\n",
    "    vocabulary[p_token_id] for p_token_id in predicted_token_ids\n",
    "  ])\n",
    "  print(\"ANOMALY DETECTED\")\n",
    "  print(\"-\" * 80)\n",
    "  print(f\"SEQUENCE:\")\n",
    "  text_wrapper = textwrap.TextWrapper()\n",
    "  sequence_lines = text_wrapper.wrap(sequence_str)\n",
    "  for line in sequence_lines:\n",
    "    print(line)\n",
    "  print(\"-\" * 80)\n",
    "  print(f\"{token_id_str}\\nNOT IN\")\n",
    "  predicted_token_ids_lines = text_wrapper.wrap(\n",
    "    predicted_token_ids_str\n",
    "  )\n",
    "  for line in predicted_token_ids_lines:\n",
    "    print(line)\n",
    "  print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def test_for_anomalies(text_sequence_analyzer: tf_models.Model,\n",
    "                       anomaly_detected_cb: abc.Callable):\n",
    "  bt_log_data = generate_bt_log(new_bt_log_gen(), SEQUENCE_LENGTH)[0]\n",
    "  # There should be no anomalies with generate_bt_log\n",
    "  # unless the model is undertrained.\n",
    "  text_sequence_analyzer.detect_anomalies(bt_log_data, anomaly_detected_cb)\n",
    "\n",
    "  bt_log_data_call_app_disconnected = (\n",
    "    \"0-unknown\\n1-unknown\\n2-unknown\\n\"\n",
    "    \"1-discovery\\n1-pairing\\n1-connected\\n\"\n",
    "    \"1-connected_hf\\n1-call_app\\n1-disconnected\"\n",
    "  )\n",
    "  text_sequence_analyzer.detect_anomalies(\n",
    "    bt_log_data_call_app_disconnected, anomaly_detected_cb\n",
    "  )\n",
    "\n",
    "  bt_log_data_call_app_discovery = (\n",
    "    \"0-unknown\\n1-unknown\\n2-unknown\\n\"\n",
    "    \"1-discovery\\n1-pairing\\n1-connected\\n\"\n",
    "    \"1-connected_hf\\n1-call_app\\n1-discovery\"\n",
    "  )\n",
    "  text_sequence_analyzer.detect_anomalies(\n",
    "    bt_log_data_call_app_discovery, anomaly_detected_cb\n",
    "  )\n",
    "\n",
    "\n",
    "test_for_anomalies(text_sequence_analyzer, on_anomaly_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwZe64Afw0IO"
   },
   "source": [
    "### Corrective action\n",
    "\n",
    "`BT_STATE_TRANSITIONS` is updated with the missing transition, so that new log generation includes the new transition and thus `sequence_model` can learn that this transition is not anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_SKjcMemw0IP"
   },
   "outputs": [],
   "source": [
    "if BT_STATE_TRANSITIONS[\"call_app\"].count(\"disconnected\") == 0:\n",
    "  BT_STATE_TRANSITIONS[\"call_app\"].append(\"disconnected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT4b3M_SP1Sn"
   },
   "source": [
    "### Incremental training and evaluation\n",
    "\n",
    "This section performs incremental training and evaluation by summarized code from other sections. Given that the code in the [Corrective action](#corrective-action) section has been executed, after some incremental training no *false positives* should be detected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SroVAv9iP1Sn"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"text_sequence_analyzer.keras\"):\n",
    "  text_sequence_analyzer = tf_models.load_model(\n",
    "    \"text_sequence_analyzer.keras\"\n",
    "  )\n",
    "  sequence_model = text_sequence_analyzer.sequence_model\n",
    "bt_log_data = generate_bt_log(bt_log_gen)\n",
    "bt_log_sequence = text_vectorizer(bt_log_data)[0]\n",
    "train_ds = to_dataset(bt_log_sequence[:-VALIDATION_SPLIT],\n",
    "                      shuffle=True, random_seed=RANDOM_SEED)\n",
    "valid_ds = to_dataset(bt_log_sequence[-VALIDATION_SPLIT:])\n",
    "train(sequence_model, train_ds, valid_ds,\n",
    "      callbacks=[early_stop_cb])\n",
    "\n",
    "test_for_anomalies(text_sequence_analyzer, on_anomaly_detected)\n",
    "text_sequence_analyzer.save(\"text_sequence_analyzer.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me7-MPuJua-G"
   },
   "source": [
    "## Global state reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xwv2vKefua-G"
   },
   "outputs": [],
   "source": [
    "tf_backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biXuS5Bfw0IP"
   },
   "source": [
    "## Towards a comprehensive end-to-end solution\n",
    "\n",
    "To develop an end-to-end solution, the mentioned in the [Motivation](#motivation) section tasks 1 and 2 can be implemented in addition to this notebook's sequential method. However, it is problematic to achieve a *generic* solution for log message filtering, classification or clusterization. Thus, as a subject to further investigation, some *customized configuration* can provide for log message filtering, classification or clusterization in a generic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMpKhlYBw0IP"
   },
   "source": [
    "## References\n",
    "\n",
    "### A survey on the application of deep learning for anomaly detection in logs\n",
    "\n",
    "<pre style=\"white-space: pre-wrap;\">\n",
    "Patrick Himler, Max Landauer, Florian Skopik, Markus Wurzenberger,\n",
    "Anomaly detection in log-event sequences: A federated deep learning approach and open challenges,\n",
    "Machine Learning with Applications,\n",
    "Volume 16,\n",
    "2024,\n",
    "100554,\n",
    "ISSN 2666-8270,\n",
    "https://doi.org/10.1016/j.mlwa.2024.100554.\n",
    "(https://www.sciencedirect.com/science/article/pii/S2666827024000306)\n",
    "Abstract: Anomaly Detection (AD) is an important area to reliably detect malicious behavior and attacks on computer systems. Log data is a rich source of information about systems and thus provides a suitable input for AD. With the sheer amount of log data available today, for years Machine Learning (ML) and more recently Deep Learning (DL) have been applied to create models for AD. Especially when processing complex log data, DL has shown some promising results in recent research to spot anomalies. It is necessary to group these log lines into log-event sequences, to detect anomalous patterns that span over multiple log lines. This work uses a centralized approach using a Long Short-Term Memory (LSTM) model for AD as its basis which is one of the most important approaches to represent long-range temporal dependencies in log-event sequences of arbitrary length. Therefore, we use past information to predict whether future events are normal or anomalous. For the LSTM model we adapt a state of the art open source implementation called LogDeep. For the evaluation, we use a Hadoop Distributed File System (HDFS) data set, which is well studied in current research. In this paper we show that without padding, which is a commonly used preprocessing step that strongly influences the AD process and artificially improves detection results and thus accuracy in lab testing, it is not possible to achieve the same high quality of results shown in literature. With the large quantity of log data, issues arise with the transfer of log data to a central entity where model computation can be done. Federated Learning (FL) tries to overcome this problem, by learning local models simultaneously on edge devices and overcome biases due to a lack of heterogeneity in training data through exchange of model parameters and finally arrive at a converging global model. Processing log data locally takes privacy and legal concerns into account, which could improve coordination and collaboration between researchers, cyber security companies, etc., in the future. Currently, there are only few scientific publications on log-based AD which use FL. Implementing FL gives the advantage of converging models even if the log data are heterogeneously distributed among participants as our results show. Furthermore, by varying individual LSTM model parameters, the results can be greatly improved. Further scientific research will be necessary to optimize FL approaches.\n",
    "Keywords: Log event sequences; Anomaly detection; Deep learning; LSTM; Federated learning\n",
    "</pre>\n",
    "\n",
    "### GitHub repos\n",
    "- [GitHub - ageron/handson-ml3: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.](https://github.com/ageron/handson-ml3)\n",
    "  - [handson-ml3/16_nlp_with_rnns_and_attention.ipynb at main · ageron/handson-ml3 · GitHub](https://github.com/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb)\n",
    "\n",
    "### Guides and tutorials\n",
    "- [Save, serialize, and export models  |  TensorFlow Core](https://www.tensorflow.org/guide/keras/serialization_and_saving#custom_objects)\n",
    "- [Text generation with an RNN  |  TensorFlow](https://www.tensorflow.org/text/tutorials/text_generation)\n",
    "- [Working with preprocessing layers  |  TensorFlow Core](https://www.tensorflow.org/guide/keras/preprocessing_layers)\n",
    "\n",
    "### Keras\n",
    "\n",
    "<pre style=\"white-space: pre-wrap;\">\n",
    "Chollet, François and others. (2015). Keras. Retrieved from https://keras.io\n",
    "</pre>\n",
    "\n",
    "- [Getting started with Keras](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility)\n",
    "\n",
    "### Matplotlib\n",
    "\n",
    "<pre style=\"white-space: pre-wrap;\">\n",
    "J. D. Hunter, \"Matplotlib: A 2D Graphics Environment,\" in Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, May-June 2007, doi: 10.1109/MCSE.2007.55. keywords: {Graphics;Interpolation;Equations;Graphical user interfaces;Packaging;Image generation;User interfaces;Operating systems;Computer languages;Programming profession;Python;scripting languages;application development;scientific programming},\n",
    "</pre>\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "<pre style=\"white-space: pre-wrap;\">\n",
    "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\n",
    "Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\n",
    "Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\n",
    "Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,\n",
    "Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,\n",
    "Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,\n",
    "Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\n",
    "Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,\n",
    "Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\n",
    "Yuan Yu, and Xiaoqiang Zheng.\n",
    "TensorFlow: Large-scale machine learning on heterogeneous systems,\n",
    "2015. Software available from tensorflow.org.\n",
    "</pre>\n",
    "\n",
    "- [Introduction to TensorFlow](https://www.tensorflow.org/learn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
